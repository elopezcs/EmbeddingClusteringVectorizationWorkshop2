{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings, Clustering and Vectorization Workshop\n",
    "Tutorial for extracting word embeddings from words.<br>\n",
    "\n",
    "**Team Members:** \n",
    "   - Edwin Lopez                     - \n",
    "   - Sabrina Ronnie George Karippatt - 8991911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vector Stores and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî§ Introduction to Word2Vec\n",
    "\n",
    "**What is Word2Vec?**\n",
    "\n",
    "Word2Vec is a popular algorithm used in Natural Language Processing (NLP) to transform words into vector representations.\n",
    "It learns these word vectors (embeddings) from a large collection of text so that words with similar meanings are located close to each other in a high-dimensional space.\n",
    "\n",
    "**Who developed it and when?**\n",
    "\n",
    "Word2Vec was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013.\n",
    "\n",
    "**Who currently maintains and supports new releases?**\n",
    "\n",
    "While the original research came from Google, the open-source Python library gensim now maintains Word2Vec functionality.\n",
    "gensim is maintained by the open-source community, originally developed by Radim ≈òeh≈Ø≈ôek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî§ Word2Vec embeddings using the Gensim library\n",
    "\n",
    "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words that capture semantic relationships between words based on their context.<br>\n",
    "As discussed, Word2Vec have 2 types, Skipgrams and CBOW. Where SkipGrams are trained to predict context words given the target word, however CBOW is trained to predict target words given its context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirements: downloading punkt from nltk, and installing gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\Users\\user\\1557_VSC\\Machin\n",
      "[nltk_data]     eLearning\\Week13_Lab3\\EmbeddingClusteringVectorization\n",
      "[nltk_data]     Workshop2\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to c:\\Users\\user\\1557_VSC\\Ma\n",
      "[nltk_data]     chineLearning\\Week13_Lab3\\EmbeddingClusteringVectoriza\n",
      "[nltk_data]     tionWorkshop2\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Paths: ['C:\\\\Users\\\\user/nltk_data', 'c:\\\\Users\\\\user\\\\1557_VSC\\\\MachineLearning\\\\Week13_Lab3\\\\.venv\\\\nltk_data', 'c:\\\\Users\\\\user\\\\1557_VSC\\\\MachineLearning\\\\Week13_Lab3\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\user\\\\1557_VSC\\\\MachineLearning\\\\Week13_Lab3\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\Users\\\\user\\\\1557_VSC\\\\MachineLearning\\\\Week13_Lab3\\\\EmbeddingClusteringVectorizationWorkshop2\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n"
     ]
    }
   ],
   "source": [
    "# importing needed libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Warning: This download will copy files to your home directory.\n",
    "# For example, on Linux, it will copy files to ~/.nltk_data.\n",
    "# In Windows, it will copy files to C:\\Users\\YourAccount\\AppData\\Roaming\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# A better way to handle the download is to:\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I use Word2Vec in Python and Jupyter Notebooks?**\n",
    "\n",
    "To use Word2Vec in Python, we typically start by following the NLP pipeline to produce a set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Sentence\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. NLP techniques aim to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n",
    "\"\"\"\n",
    "tokenized_words = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a model in Word2Vec?**\n",
    "\n",
    "A Word2Vec model is a trained neural network that maps words from your dataset to numerical vectors (embeddings).\n",
    "The model learns word relationships based on how often they appear together in context.\n",
    "\n",
    "After the model is trained, each word in your vocabulary is now represented by a vector of numbers that can be used in tasks like similarity, clustering, or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† Explanation of the model creation instruction\n",
    "\n",
    "- `sentences`: A list of tokenized sentences (in this case, [tokenized_words] means a list of words).\n",
    "- `vector_size=100`: Each word will be represented by a vector with 100 dimensions.\n",
    "- `window=5`: The model considers 5 words before and after the target word (context window).\n",
    "- `min_count=1`: Include all words that appear at least once.\n",
    "- `workers=4`: Uses 4 CPU threads to speed up training.\n",
    "\n",
    "**This model will learn how words relate to each other and store them as vectors in its internal memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model_word2Vec = Word2Vec(\n",
    "    sentences=[tokenized_words], vector_size=100, window=5, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'natural' using Word2Vec: [-1.9428101e-03 -5.2659796e-03  9.4477097e-03 -9.2954319e-03\n",
      "  4.5052692e-03  5.4033557e-03 -1.4071489e-03  9.0089496e-03\n",
      "  9.8826429e-03 -5.4782177e-03 -6.0212663e-03 -6.7517818e-03\n",
      " -7.8889774e-03 -3.0443431e-03 -5.5920784e-03 -8.3447993e-03\n",
      "  7.8853959e-04  2.9931944e-03  6.4117215e-03 -2.6343642e-03\n",
      " -4.4501973e-03  1.2521694e-03  3.9400163e-04  8.1163580e-03\n",
      "  1.8195010e-04  7.2335410e-03 -8.2651470e-03  8.4343469e-03\n",
      " -1.8896088e-03  8.7051662e-03 -7.6160980e-03  1.7950600e-03\n",
      "  1.0553870e-03  3.8771374e-05 -5.1055676e-03 -9.2426352e-03\n",
      " -7.2625973e-03 -7.9506449e-03  1.9089831e-03  4.7897184e-04\n",
      " -1.8130449e-03  7.1189236e-03 -2.4739422e-03 -1.3462100e-03\n",
      " -8.8952789e-03 -9.9256253e-03  8.9443158e-03 -5.7570883e-03\n",
      " -6.3713118e-03  5.1996382e-03  6.6684661e-03 -6.8323021e-03\n",
      "  9.5778168e-04 -6.0134367e-03  1.6455727e-03 -4.2872936e-03\n",
      " -3.4413836e-03  2.1873817e-03  8.6617842e-03  6.7299707e-03\n",
      " -9.6773999e-03 -5.6279455e-03  7.8832312e-03  1.9950410e-03\n",
      " -4.2571262e-03  6.0163322e-04  9.5161879e-03 -1.0983507e-03\n",
      " -9.4264010e-03  1.6100770e-03  6.2353830e-03  6.2832576e-03\n",
      "  4.0935059e-03 -5.6479434e-03 -3.7046376e-04 -5.6182642e-05\n",
      "  4.5747375e-03 -8.0409022e-03 -8.0180895e-03  2.6604361e-04\n",
      " -8.6128768e-03  5.8176308e-03 -4.1866081e-04  9.9716838e-03\n",
      " -5.3435448e-03 -4.8722714e-04  7.7595441e-03 -4.0666410e-03\n",
      " -5.0089173e-03  1.5904332e-03  2.6518397e-03 -2.5642074e-03\n",
      "  6.4473739e-03 -7.6613198e-03  3.3946137e-03  4.9176463e-04\n",
      "  8.7319044e-03  5.9785498e-03  6.8169599e-03  7.8231385e-03]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "word = \"natural\"\n",
    "vector_word2Vec = model_word2Vec.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_word2Vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### üìä Understanding Word Vectors in Word2Vec\n",
    "\n",
    "#### üî¢ Code Recap\n",
    "\n",
    "```python\n",
    "# Get word vector for a specific word\n",
    "word = \"natural\"\n",
    "vector_word2Vec = model_word2Vec.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_word2Vec}\")\n",
    "```\n",
    "\n",
    "This code retrieves the **vector representation (embedding)** of the word `\"natural\"` from the trained Word2Vec model.\n",
    "\n",
    "#### üß† What Is a Word Vector?\n",
    "\n",
    "In **Word2Vec**, every word in your vocabulary is represented by a **dense vector** of real numbers.\n",
    "\n",
    "* Each word becomes a **point** in a high-dimensional space.\n",
    "* Words that appear in similar contexts are placed **closer together**.\n",
    "* These vectors are learned by a shallow neural network during training.\n",
    "\n",
    "The output you see is a **vector with 100 dimensions**, because we set `vector_size=100` when training the model.\n",
    "\n",
    "#### ‚úçÔ∏è What Do These Numbers Mean?\n",
    "\n",
    "The output:\n",
    "\n",
    "```python\n",
    "[ 9.7677782e-03, 8.1660571e-03, ..., -2.3143364e-03 ]\n",
    "```\n",
    "\n",
    "...is a list of 100 floating-point numbers, like:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{\\text{natural}} = [v_1, v_2, v_3, \\ldots, v_{100}]\n",
    "$$\n",
    "\n",
    "This vector encodes the **semantic meaning** of the word *natural* based on its context in the training data.\n",
    "\n",
    "While **each individual number doesn't mean anything by itself**, together they represent a **position** in a 100-dimensional space.\n",
    "\n",
    "#### üîç Why Is This Useful?\n",
    "\n",
    "You can perform various mathematical operations with these vectors:\n",
    "\n",
    "#### **Similarity** between words\n",
    "\n",
    "We can compute **cosine similarity** to check how similar two words are:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\vec{v}_a, \\vec{v}_b) = \\frac{\\vec{v}_a \\cdot \\vec{v}_b}{\\|\\vec{v}_a\\| \\|\\vec{v}_b\\|}\n",
    "$$\n",
    "\n",
    "If two word vectors point in a similar direction (small angle), they are semantically similar.\n",
    "\n",
    "#### üßÆ Summary\n",
    "\n",
    "* The output is a **100-dimensional vector** for the word `\"natural\"`.\n",
    "* Each dimension is a learned number that helps position the word in a semantic space.\n",
    "* These vectors are powerful tools for **clustering**, **search**, **recommendations**, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'natural' using Word2Vec: [('subfield', 0.2477034479379654), ('focuses', 0.23781010508537292), ('.', 0.15014639496803284), ('interpret', 0.1489964723587036), ('nlp', 0.1281409114599228), ('enable', 0.1139271929860115), ('humans', 0.09733925759792328), ('and', 0.09313160181045532), ('contextually', 0.0922219306230545), ('intelligence', 0.0911305621266365)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words_word2Vec = model_word2Vec.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_word2Vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Finding Similar Words with Word2Vec\n",
    "\n",
    "#### üî¢ Code Recap\n",
    "\n",
    "```python\n",
    "# Find similar words\n",
    "similar_words_word2Vec = model_word2Vec.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_word2Vec}\")\n",
    "```\n",
    "\n",
    "This code retrieves the **top 10 words** that are most similar to the word `\"natural\"` according to the Word2Vec model.\n",
    "\n",
    "#### üß† What Does ‚ÄúSimilar‚Äù Mean in Word2Vec?\n",
    "\n",
    "Word2Vec considers words to be similar if their **vector representations** are **close together** in high-dimensional space.\n",
    "\n",
    "This closeness is measured using **cosine similarity**:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\vec{v}_a, \\vec{v}_b) = \\frac{\\vec{v}_a \\cdot \\vec{v}_b}{\\|\\vec{v}_a\\| \\|\\vec{v}_b\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\vec{v}_a$ and $\\vec{v}_b$ are the word vectors.\n",
    "* $\\cdot$ is the dot product.\n",
    "* $\\|\\vec{v}\\|$ is the magnitude (length) of vector $\\vec{v}$.\n",
    "\n",
    "This produces a value between **-1 and 1**:\n",
    "\n",
    "* **1** ‚Üí perfectly similar (same direction)\n",
    "* **0** ‚Üí no similarity (orthogonal)\n",
    "* **-1** ‚Üí completely opposite\n",
    "\n",
    "#### üì§ Example Output Explained\n",
    "\n",
    "```python\n",
    "[('the', 0.182), ('computers', 0.173), ('nlp', 0.167), ('between', 0.156), ...]\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "| Word       | Cosine Similarity |\n",
    "| ---------- | ----------------: |\n",
    "| the        |             0.182 |\n",
    "| computers  |             0.173 |\n",
    "| nlp        |             0.167 |\n",
    "| between    |             0.156 |\n",
    "| way        |             0.133 |\n",
    "| techniques |             0.122 |\n",
    "| human      |             0.112 |\n",
    "| is         |             0.111 |\n",
    "| .          |             0.109 |\n",
    "| in         |             0.097 |\n",
    "\n",
    "These words appeared in **similar contexts** to `\"natural\"` in the training data.\n",
    "\n",
    "üí° Example: If the training sentences included phrases like *\"natural language processing\"*, and also had *\"nlp techniques\"*, *\"human language\"*, or *\"way computers process language\"*, then `\"nlp\"`, `\"human\"`, `\"techniques\"`, and `\"computers\"` would naturally be nearby in vector space.\n",
    "\n",
    "#### üéØ Summary\n",
    "\n",
    "* Word2Vec learned that these words tend to appear **in similar contexts** to `\"natural\"`.\n",
    "* The similarity is calculated using **cosine similarity** on the word vectors.\n",
    "* The output is a **ranked list** of the most contextually similar words.\n",
    "\n",
    "You can use this method for:\n",
    "\n",
    "* **Synonym detection**\n",
    "* **Query expansion in search engines**\n",
    "* **Exploring semantic relationships in text**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'natural' using Word2Vec: [-1.9321169e-03 -5.2483547e-03  9.4544906e-03 -9.2837224e-03\n",
      "  4.4994610e-03  5.3914869e-03 -1.3959594e-03  9.0288678e-03\n",
      "  9.8756803e-03 -5.4979529e-03 -6.0152854e-03 -6.7671919e-03\n",
      " -7.8648757e-03 -3.0349847e-03 -5.5805137e-03 -8.3396826e-03\n",
      "  8.1575551e-04  2.9894954e-03  6.3903220e-03 -2.6621453e-03\n",
      " -4.4344361e-03  1.2641826e-03  4.0435637e-04  8.1096366e-03\n",
      "  1.6329886e-04  7.2371974e-03 -8.2724625e-03  8.4320260e-03\n",
      " -1.8909355e-03  8.7151248e-03 -7.6115341e-03  1.7841731e-03\n",
      "  1.0538050e-03  8.1357475e-06 -5.1192981e-03 -9.2276307e-03\n",
      " -7.2482466e-03 -7.9528000e-03  1.8848567e-03  4.7819575e-04\n",
      " -1.8095361e-03  7.1224878e-03 -2.4662046e-03 -1.3527130e-03\n",
      " -8.8796709e-03 -9.9325730e-03  8.9337984e-03 -5.7706139e-03\n",
      " -6.3643786e-03  5.2108620e-03  6.6559934e-03 -6.8443152e-03\n",
      "  9.4641995e-04 -6.0298033e-03  1.6457191e-03 -4.2782631e-03\n",
      " -3.4359298e-03  2.1962207e-03  8.6636264e-03  6.7324229e-03\n",
      " -9.6782390e-03 -5.6465934e-03  7.8966795e-03  2.0118570e-03\n",
      " -4.2676581e-03  6.2705070e-04  9.5009860e-03 -1.0856916e-03\n",
      " -9.4406707e-03  1.6132817e-03  6.2452569e-03  6.2859505e-03\n",
      "  4.1067773e-03 -5.6415065e-03 -3.6664642e-04 -5.4928030e-05\n",
      "  4.5822519e-03 -8.0356579e-03 -8.0316104e-03  2.6767500e-04\n",
      " -8.6301845e-03  5.8207465e-03 -4.2043763e-04  9.9862562e-03\n",
      " -5.3365268e-03 -4.9435132e-04  7.7727772e-03 -4.0548993e-03\n",
      " -4.9905884e-03  1.5938597e-03  2.6562414e-03 -2.5683788e-03\n",
      "  6.4470088e-03 -7.6677208e-03  3.4062741e-03  4.9559533e-04\n",
      "  8.7263258e-03  5.9645511e-03  6.8198047e-03  7.8309029e-03]\n",
      "Similar words to 'natural' using Word2Vec: [('subfield', 0.24804812669754028), ('focuses', 0.2379235476255417), ('.', 0.14982585608959198), ('interpret', 0.1489376276731491), ('nlp', 0.12802208960056305), ('enable', 0.11342129111289978), ('humans', 0.09785904735326767), ('and', 0.09353728592395782), ('contextually', 0.0924823209643364), ('intelligence', 0.09197212755680084)]\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model_skipGram = Word2Vec(\n",
    "    sentences=[tokenized_words], vector_size=100, window=5, min_count=1, workers=4, sg=1\n",
    ")\n",
    "\n",
    "# Get word vector for a specific word\n",
    "vector_skipGram = model_skipGram.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_skipGram}\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words_skipGrams = model_skipGram.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_skipGrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† Understanding Skip-grams in Word2Vec\n",
    "\n",
    "### üîÅ What Are Skip-grams?\n",
    "\n",
    "A **Skip-gram** is a training approach used in Word2Vec that **predicts context words given a center word**.\n",
    "\n",
    "> üîë Goal: For a given target word, predict the words that are likely to appear nearby in a sentence.\n",
    "\n",
    "#### üìö How Does Skip-gram Work?\n",
    "\n",
    "Let‚Äôs take a sentence:\n",
    "\n",
    "```\n",
    "\"The field of natural language processing is growing.\"\n",
    "```\n",
    "\n",
    "If the center word is `\"natural\"` and the window size is 2, the context window is:\n",
    "\n",
    "```python\n",
    "[\"of\", \"natural\", \"language\", \"processing\"]\n",
    "```\n",
    "\n",
    "The skip-gram model will create training pairs like:\n",
    "\n",
    "```python\n",
    "(\"natural\", \"of\"), (\"natural\", \"language\")\n",
    "```\n",
    "<br/>\n",
    "\n",
    "üß† The model **learns embeddings** by trying to **maximize the probability** of seeing context words given the center word.\n",
    "\n",
    "#### üìê The Skip-gram Objective Function\n",
    "\n",
    "The skip-gram model aims to maximize the following log-likelihood objective over a large corpus:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $T$ is the total number of words in the corpus\n",
    "* $w_t$ is the center word\n",
    "* $w_{t+j}$ are the surrounding context words\n",
    "* $c$ is the window size\n",
    "\n",
    "The probability $P(w_{t+j} \\mid w_t)$ is computed using softmax:\n",
    "\n",
    "$$\n",
    "P(w_O \\mid w_I) = \\frac{\\exp\\left({\\vec{v}_{w_O}^\\top \\vec{v}_{w_I}}\\right)}{\\sum_{w = 1}^{V} \\exp\\left({\\vec{v}_w^\\top \\vec{v}_{w_I}}\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\vec{v}_{w_I}$: vector of the input word (center)\n",
    "* $\\vec{v}_{w_O}$: vector of the output/context word\n",
    "* $V$: vocabulary size\n",
    "\n",
    "### üß™ Code Recap\n",
    "\n",
    "```python\n",
    "model_skipGram = Word2Vec(sentences=[tokenized_words], vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "```\n",
    "\n",
    "* `sg=1` activates the **Skip-gram** architecture (`sg=0` would activate CBOW instead).\n",
    "* This model now learns word embeddings by training on `(center ‚Üí context)` word pairs.\n",
    "\n",
    "#### üì§ Output Explanation\n",
    "\n",
    "#### ‚úÖ Word Vector\n",
    "\n",
    "```python\n",
    "vector_skipGram = model_skipGram.wv[word]\n",
    "```\n",
    "\n",
    "Returns a **100-dimensional vector** for the word `\"natural\"` that reflects its learned representation based on surrounding context words.\n",
    "\n",
    "#### ‚úÖ Most Similar Words\n",
    "\n",
    "```python\n",
    "model_skipGram.wv.most_similar(\"natural\")\n",
    "```\n",
    "\n",
    "Returns a list of the **top 10 most similar words** to `\"natural\"`, ranked by **cosine similarity** between their vectors.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```python\n",
    "[('the', 0.182), ('computers', 0.173), ('nlp', 0.168), ...]\n",
    "```\n",
    "\n",
    "These are the words that appeared in **similar contexts** to `\"natural\"` across the training data.\n",
    "<br/>\n",
    "\n",
    "üß† This similarity emerges from the way skip-gram **updates** the vector for `\"natural\"` and its surrounding words whenever they co-occur ‚Äî over time, semantically related words move closer together in vector space.\n",
    "\n",
    "#### üß≠ Summary\n",
    "\n",
    "| Component           | Role                                                     |\n",
    "| ------------------- | -------------------------------------------------------- |\n",
    "| Skip-gram Objective | Predicts context words based on a center word            |\n",
    "| sg=1                | Enables skip-gram architecture in `Word2Vec()`           |\n",
    "| Output Vector       | A 100D numeric representation of the word \"natural\"      |\n",
    "| Similar Words       | Top 10 words that share similar context to \"natural\"     |\n",
    "| Math Mechanism      | Uses dot products and softmax to update vector positions |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipGram.save(\"saveModelSkipGram.bin\")\n",
    "# Word2Vec.load('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíæ Saving and Loading a Word2Vec Skip-gram Model\n",
    "\n",
    "#### ‚úÖ What This Code Does\n",
    "\n",
    "#### üìå `model_skipGram.save('saveModelSkipGram.bin')`\n",
    "\n",
    "* This **saves** the trained Word2Vec Skip-gram model to a file named `'saveModelSkipGram.bin'`.\n",
    "* You can later load this file to **reuse the model** without retraining it.\n",
    "* Useful for large models that take time to train.\n",
    "\n",
    "#### üìå `Word2Vec.load('path')`\n",
    "\n",
    "* This **loads** a previously saved Word2Vec model from disk.\n",
    "* You can then use the loaded model to get vectors or find similar words.\n",
    "\n",
    "#### üß† Example\n",
    "\n",
    "```python\n",
    "# Save the model\n",
    "model_skipGram.save('saveModelSkipGram.bin')\n",
    "\n",
    "# Later or in another notebook\n",
    "from gensim.models import Word2Vec\n",
    "loaded_model = Word2Vec.load('saveModelSkipGram.bin')\n",
    "\n",
    "# Use the loaded model\n",
    "loaded_model.wv['natural']\n",
    "```\n",
    "\n",
    "#### üóÇÔ∏è Summary\n",
    "\n",
    "| Action    | What It Does                      |\n",
    "| --------- | --------------------------------- |\n",
    "| `.save()` | Saves the trained model to a file |\n",
    "| `.load()` | Loads the model back into memory  |\n",
    "\n",
    "\n",
    "\n",
    "üî• Important Practical Example\n",
    "\n",
    "Imagine you have a pretrained Word2Vec on news articles, but your dataset is medical research papers.\n",
    "\n",
    "Pretrained Word2Vec does not understand words like angiogenesis or fibrosis.\n",
    "\n",
    "It may misrepresent words like \"cell\", which have different meanings in common English vs. biology.\n",
    "\n",
    "‚û°Ô∏è Retraining or fine-tuning improves performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use word2Vec for SkipGrams and CBOW? Explore whether they will give different results for similar words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Clustering Words based on Cooccurence Pattern - **Brown Clustering**\n",
    "\n",
    "> Brown Clustering is a method to cluster words based on their co-occurrence patterns.\n",
    "\n",
    "> It starts with each word as a separate cluster and iteratively merges the most similar clusters based on their co-occurrence patterns.\n",
    "\n",
    "> Brown Clustering is used for tasks like named entity recognition, word sense disambiguation, and topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Corpus Clustering Tutorial\n",
    "\n",
    "In this notebook, we will explore **word clustering** using the Brown corpus from NLTK. The goal is to visualize clusters of words based on their **co-occurrence patterns** using a **dendrogram**.\n",
    "\n",
    "---\n",
    "\n",
    "### Concepts Covered\n",
    "\n",
    "1. **Brown Corpus**: A large collection of English text samples from 1961. It contains text from multiple genres, useful for NLP experiments.\n",
    "\n",
    "2. **Vocabulary and Co-occurrence**:\n",
    "   - Vocabulary (`vocab`): The set of unique words in a corpus.\n",
    "   - Co-occurrence matrix: A square matrix where each entry `(i, j)` counts how often word `i` appears near word `j` within a defined **window size**.\n",
    "\n",
    "3. **Word Clustering**:\n",
    "   - Words that appear in similar contexts tend to have similar meanings.\n",
    "   - We can use the co-occurrence matrix to cluster words hierarchically.\n",
    "\n",
    "4. **Hierarchical Clustering & Dendrograms**:\n",
    "   - **Hierarchical clustering** groups words step-by-step based on similarity.\n",
    "   - A **dendrogram** is a tree diagram that shows the hierarchy of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 0 ‚Äì Imports & Environment Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download NLTK resources (run once; it will be cached)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"Environment ready \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Corpus\n",
    "\n",
    "- We will use a subset of the Brown corpus for faster computation.\n",
    "- All words are converted to lowercase to avoid duplicates due to capitalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\user/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download the Brown corpus (only needed once)\n",
    "nltk.download(\"brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first 50 sentences from Brown corpus\n",
    "corpus = brown.sents()[:50]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3 ‚Äì Build Vocabulary**\n",
    "\n",
    "The vocabulary is the set of all distinct words that appear in our corpus.\n",
    "\n",
    "From the tokenized Brown sentences, we will:\n",
    "\n",
    "   - Flatten all sentences into a single list of tokens.\n",
    "   - Count word frequencies using `collections.Counter`.\n",
    "   - Build:\n",
    "     * a **sorted vocabulary list** `vocab`\n",
    "     * a `word_to_id` mapping (word ‚Üí integer index)\n",
    "     * an `id_to_word` mapping (integer index ‚Üí word)\n",
    "\n",
    "These structures will be used later for:\n",
    "\n",
    "  - Building **co-occurrence matrices** (for GloVe-style training).\n",
    "  - Inspecting and interpreting **word embedding vectors**.\n",
    "\n",
    "**Talking point:**  \n",
    "  - Even this small slice of the Brown corpus already contains noisy spelling,\n",
    "  - function words, and rare terms. In a larger project, we would typically\n",
    "  - drop very rare words or apply additional normalization (e.g., lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens (with repetition): 1150\n",
      "Unique tokens (vocabulary size): 478\n",
      "\n",
      "First 15 words in vocabulary:\n",
      "  \"''\" ‚Üí id 0\n",
      "  '(' ‚Üí id 1\n",
      "  ')' ‚Üí id 2\n",
      "  ',' ‚Üí id 3\n",
      "  '--' ‚Üí id 4\n",
      "  '.' ‚Üí id 5\n",
      "  '1' ‚Üí id 6\n",
      "  '13' ‚Üí id 7\n",
      "  '1913' ‚Üí id 8\n",
      "  '1923' ‚Üí id 9\n",
      "  '1937' ‚Üí id 10\n",
      "  '1962' ‚Üí id 11\n",
      "  '2' ‚Üí id 12\n",
      "  '637' ‚Üí id 13\n",
      "  '71' ‚Üí id 14\n",
      "\n",
      "Top 10 most frequent tokens:\n",
      "  'the': 90\n",
      "  '.': 45\n",
      "  ',': 41\n",
      "  'of': 37\n",
      "  'and': 28\n",
      "  '``': 24\n",
      "  \"''\": 23\n",
      "  'in': 21\n",
      "  'to': 21\n",
      "  'a': 20\n"
     ]
    }
   ],
   "source": [
    "# Step 3 ‚Äì Build vocabulary from the Brown corpus slice\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for sentence in corpus for token in sentence]\n",
    "\n",
    "print(f\"Total tokens (with repetition): {len(all_tokens)}\")\n",
    "\n",
    "# Count frequencies\n",
    "vocab_counts = Counter(all_tokens)\n",
    "print(f\"Unique tokens (vocabulary size): {len(vocab_counts)}\")\n",
    "\n",
    "# For a tiny corpus, we keep all words.\n",
    "# For larger corpora, we might filter: keep only words with count >= min_freq.\n",
    "# min_freq = 2\n",
    "# vocab = sorted([w for w, c in vocab_counts.items() if c >= min_freq])\n",
    "\n",
    "vocab = sorted(vocab_counts.keys())\n",
    "\n",
    "# Build index mappings\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "print(\"\\nFirst 15 words in vocabulary:\")\n",
    "for w in vocab[:15]:\n",
    "    print(f\"  {w!r} ‚Üí id {word_to_id[w]}\")\n",
    "\n",
    "print(\"\\nTop 10 most frequent tokens:\")\n",
    "for w, c in vocab_counts.most_common(10):\n",
    "    print(f\"  {w!r}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3 ‚Äì Summary**\n",
    "\n",
    "The Brown corpus slice (first 50 sentences) contains **1,150 total tokens** and **478 unique tokens**, including numbers, punctuation marks, and proper nouns. This demonstrates the natural variability of real-world text data.\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "   - **Flattened** all sentences into a single list of tokens.\n",
    "   - **Counted word frequencies** using collections.Counter.\n",
    "   - Constructed:\n",
    "     * a **sorted vocabulary list** (vocab)\n",
    "     * a word_to_id dictionary mapping each word to a unique index  \n",
    "     * an id_to_word dictionary for reverse lookup\n",
    "\n",
    "Sorting the vocabulary ensures **stable and reproducible indexing**, which is important when constructing co-occurrence matrices or interpreting embedding vectors.\n",
    "\n",
    "From the frequency output, we observe:\n",
    "\n",
    "   - Common English words like **‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúin‚Äù, ‚Äúto‚Äù** dominate the corpus.\n",
    "   - Punctuation marks (e.g., ., ,, '') appear frequently because the Brown corpus preserves original sentence structure.\n",
    "   - Rare tokens, such as **year references** (‚Äú1913‚Äù, ‚Äú1937‚Äù, etc.), each appear only once.\n",
    "\n",
    "These observations illustrate two important NLP considerations:\n",
    "\n",
    "- **Frequent words** contribute heavily to co-occurrence statistics and influence embedding geometry.\n",
    "- **Rare words** add vocabulary size but contribute little information; in larger corpora, they are often removed using a minimum-frequency threshold.\n",
    "\n",
    "Overall, this vocabulary serves as the foundation for the next steps, where we will build **context windows**, a **co-occurrence matrix**, and later  \n",
    "train **Word2Vec** and **GloVe** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   - The vocabulary extracted from the Brown corpus is relatively small (478 words), which makes it manageable for co-occurrence and clustering experiments.  \n",
    "   - However, small vocabularies lead to sparse vectors, and rare words may not have meaningful co-occurrence patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4 ‚Äì Construct the Co-occurrence Matrix**\n",
    "\n",
    "To model word meaning based on the *distributional hypothesis* (‚Äúwords appearing in similar contexts tend to have similar meanings‚Äù), we construct a **co-occurrence matrix**.\n",
    "\n",
    "**What we do in this step:**\n",
    "\n",
    "   - Slide a **window** across each sentence.\n",
    "   - For each center word, count how often nearby words appear within a fixed window size (here we use `window_size = 2`).\n",
    "   - Build a `V √ó V` matrix, where `V` is the vocabulary size.\n",
    "   - Entry `(i, j)` = number of times word `j` appears within the window of word `i`.\n",
    "\n",
    "This matrix is the foundation for **GloVe** and other count-based embedding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix shape: (478, 478)\n",
      "Sample row for 'the': [ 8  0  0 18  0  1  1  1  0  0  0  1  0  0  0  0  0  0  4  0]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 ‚Äì Construct co-occurrence matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "window_size = 2\n",
    "V = len(vocab)\n",
    "\n",
    "co_matrix = np.zeros((V, V), dtype=np.int32)\n",
    "\n",
    "for sentence in corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        wi = word_to_id[word]   # <-- updated to match Step 3\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sentence), i + window_size + 1)\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if j == i:\n",
    "                continue  # skip the center word itself\n",
    "            cj = word_to_id[sentence[j]]\n",
    "            co_matrix[wi, cj] += 1\n",
    "\n",
    "print(\"Co-occurrence matrix shape:\", co_matrix.shape)\n",
    "print(\"Sample row for 'the':\", co_matrix[word_to_id.get(\"the\", 0)][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4 ‚Äì Summary**\n",
    "\n",
    "The co-occurrence matrix was successfully constructed with a shape of 478 √ó 478, matching the vocabulary size derived in Step 3.\n",
    "\n",
    "Each row represents a center word, and each column represents a context word. Entry (i, j) stores how many times word j appeared within a window of size 2 around word i in the Brown corpus slice.\n",
    "\n",
    "A sample inspection of the row for the word ‚Äúthe‚Äù shows values such as:\n",
    "\n",
    "[ 8, 0, 0, 18, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, ... ]\n",
    "\n",
    "\n",
    "This indicates:\n",
    "\n",
    "- ‚Äúthe‚Äù frequently appears near punctuation marks and common nouns (e.g., commas, periods, early sentence tokens).\n",
    "- Many cells are zero, reflecting the natural sparsity of co-occurrence matrices in real-world text.\n",
    "- Frequent words generate stronger context patterns, while rare words produce sparse, nearly empty rows.\n",
    "\n",
    "This matrix captures essential distributional structure in the corpus and forms the foundation for GloVe embeddings, which rely on global co-occurrence statistics rather than predictive context sampling (as used in Word2Vec).\n",
    "\n",
    "The next step is to train a Word2Vec model to learn predictive embeddings based on local context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   - The co-occurrence matrix is extremely sparse because most word pairs never appear near each other. This sparsity is the main reason why raw co-occurrence counts are hard to use directly and motivates dimensionality reduction techniques such as GloVe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5 ‚Äì Hierarchical Clustering (Ward's Method)**\n",
    "\n",
    "In this step, we apply Agglomerative Hierarchical Clustering using Ward‚Äôs method, which groups words so that each merge minimizes the  \n",
    "increase in within-cluster variance.\n",
    "\n",
    "Because Ward‚Äôs method operates on distance matrices, we first convert the co-occurrence matrix into a distance representation. A common  \n",
    "approach is to compute:\n",
    "- Cosine distance between word co-occurrence vectors\n",
    "\n",
    "This gives a more meaningful similarity measure for distributional data.\n",
    "\n",
    "The resulting linkage matrix can be visualized using a dendrogram to inspect how words group together based on shared contextual behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linkage matrix shape: (477, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5 ‚Äì Hierarchical Clustering using Ward's method\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute pairwise cosine distances between word vectors\n",
    "dist_matrix = pdist(co_matrix, metric='cosine')\n",
    "\n",
    "# Perform hierarchical clustering (Ward's method)\n",
    "linkage_matrix = linkage(dist_matrix, method=\"ward\")\n",
    "\n",
    "print(\"Linkage matrix shape:\", linkage_matrix.shape)\n",
    "\n",
    "# Plot a small dendrogram for first 30 words to avoid clutter\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=vocab,\n",
    "    truncate_mode=\"lastp\",  # compress deep clusters\n",
    "    p=20,                   # show only top clusters\n",
    "    leaf_rotation=90,\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering of Words (Ward‚Äôs Method)\")\n",
    "plt.xlabel(\"Words / Combined Clusters\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5 ‚Äì Summary**\n",
    "\n",
    "The hierarchical clustering successfully produced a linkage matrix of shape (477 √ó 4), which corresponds to:\n",
    "\n",
    "   - 477 merge operations (because clustering a vocabulary of 478 words requires V-1 merges)\n",
    "   - Each row representing:\n",
    "     * cluster/word index 1  \n",
    "     * cluster/word index 2  \n",
    "     * distance at which they were merged  \n",
    "     * size of the newly formed cluster  \n",
    "\n",
    "To visualize the structure, we plotted a truncated dendrogram using the top 20 clusters for readability. The dendrogram shows:\n",
    "\n",
    "- Early (low-distance) merges between words with very similar co-occurrence distributions, such as common function words and punctuation marks.\n",
    "- Larger merges happening at higher distances, representing groups of words that share only broad contextual similarities.\n",
    "- The tall vertical lines near the top of the plot indicate clusters that are highly distinct from one another, demonstrating meaningful structure  \n",
    "  even from this small slice of the Brown corpus.\n",
    "\n",
    "This confirms that the distributional patterns captured in the co-occurrence matrix are strong enough to produce coherent word clusters, even before  \n",
    "learning embeddings. Ward‚Äôs method helps visualize these relationships by progressively joining the most similar words and clusters.\n",
    "\n",
    "The hierarchical clustering step builds an important conceptual bridge between raw co-occurrence features and the dense vector embeddings\n",
    "we learn next using Word2Vec and GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   -  Ward‚Äôs method was chosen because it minimizes the variance within clusters, making the hierarchy more interpretable. Using other linkage methods (e.g., single or complete linkage) would produce very different cluster shapes and potentially less meaningful merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6 ‚Äì Visualizing the Hierarchical Clustering (Dendrogram)**\n",
    "\n",
    "A dendrogram provides a visual representation of the hierarchical clustering performed using Ward‚Äôs method. Each leaf represents a word, and words that merge at smaller distances have more similar co-occurrence patterns.\n",
    "\n",
    "Since the vocabulary contains 478 words, plotting the full dendrogram can become cluttered and difficult to interpret. Therefore, we visualize a \n",
    "truncated dendrogram, which shows only the major clusters at higher levels while preserving the overall hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6 ‚Äì Visualize dendrogram (truncated for readability)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=vocab,\n",
    "    truncate_mode=\"lastp\",  # show only top-level clusters\n",
    "    p=25,                   # number of clusters to display\n",
    "    leaf_rotation=0,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=None\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Cluster Groups\")\n",
    "plt.title(\"Truncated Dendrogram ‚Äì Hierarchical Clustering of Brown Corpus Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6 ‚Äì Summary** \n",
    "\n",
    "The truncated dendrogram provides a clear and interpretable view of the major clusters produced using Ward‚Äôs hierarchical clustering method. Instead of displaying all 478 vocabulary words‚Äîwhich would result in an unreadable plot‚Äîthe visualization shows the top 25 cluster merges, summarizing the higher-level structure of the corpus.\n",
    "\n",
    "From the dendrogram, we observe:\n",
    "\n",
    "- Several clusters merge at low distances, indicating groups of words that share very similar co-occurrence profiles in the Brown sentences.\n",
    "- Mid-level merges form larger clusters composed of these smaller groups, reflecting broader patterns of contextual similarity.\n",
    "- A few merges occur at significantly higher distances, shown by the taller vertical lines on the right side of the plot. These represent clusters that are contextually more distinct from the rest of the vocabulary.\n",
    "- The overall hierarchical structure demonstrates how local context statistics naturally arrange words into meaningful groupings before applying any \n",
    "  embedding model.\n",
    "\n",
    "This visualization confirms that the co-occurrence matrix contains rich distributional information, and hierarchical clustering reveals meaningful relationships between words. These structural patterns provide an intuitive foundation for learning dense vector embeddings using Word2Vec and GloVe in the upcoming steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "\n",
    "   - Using the Brown Corpus, we extracted a real-world sample of sentences and constructed a complete vocabulary of 478 unique tokens.\n",
    "   - A co-occurrence matrix was built using a sliding context window of size 2, capturing how often words appear near each other. This matrix forms the basis for many classical distributional semantics models.\n",
    "   - We applied hierarchical clustering (Ward‚Äôs method) to group words based on similarities in their co-occurrence patterns. The resulting linkage matrix revealed meaningful structure, demonstrating how words cluster together according to shared contextual behavior.\n",
    "   - A truncated dendrogram was used to visualize the top-level clusters, providing an interpretable view of major word groupings without overwhelming the reader with all 478 leaves.\n",
    "   - These steps collectively demonstrate how raw text can be transformed into structured numerical representations that expose linguistic patterns.  \n",
    "  This workflow serves as a foundational pipeline for more advanced embedding techniques such as Word2Vec, GloVe, and other vector-based \n",
    "  distributional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a Co-occurrence Matrix**\n",
    "\n",
    "We build a **co-occurrence matrix** using a sliding window of size 2. For each word in each sentence of the Brown corpus, we count how often  \n",
    "nearby words appear within this window.\n",
    "\n",
    "```python\n",
    "window_size = 2\n",
    "co_matrix = np.zeros((len(vocab), len(vocab)), dtype=np.int32)\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "The matrix is square with shape (vocab_size √ó vocab_size).\n",
    "\n",
    "Entry (i, j) stores how many times word j appears within the ¬±2-word context window of word i.\n",
    "\n",
    "We use word_to_id to map each word to its correct row/column index.\n",
    "\n",
    "This matrix captures the distributional structure of the corpus and is used in the next steps for hierarchical clustering and later for embedding models such as Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the Co-occurrence Matrix and Clustering Process**\n",
    "\n",
    "In this notebook, we do **not** implement classical Brown Clustering.  \n",
    "Instead, we use the co-occurrence matrix to perform:\n",
    "\n",
    "**Hierarchical Agglomerative Clustering**  \n",
    "   - Using **Ward‚Äôs method**  \n",
    "   - Followed by a **truncated dendrogram visualization**\n",
    "\n",
    "The steps are:\n",
    "\n",
    "**Build the Co-occurrence Matrix**\n",
    "   - Rows and columns represent the **vocabulary words**.\n",
    "   - Each cell \\( M_{i,j} \\) counts how many times word \\( j \\) appears within a ¬±2-word context window of word \\( i \\).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "\\[\n",
    "M_{i,j} = \\text{Number of times word } w_j \\text{ appears near } w_i\n",
    "\\]\n",
    "\n",
    "This matrix captures the **distributional structure** of words in the Brown corpus.\n",
    "\n",
    "**Apply Hierarchical Clustering (Ward‚Äôs Method)**\n",
    "   - Each word starts as its own cluster.\n",
    "   - Words with **similar co-occurrence patterns** are merged first.\n",
    "   - The process continues until all words are part of a single hierarchy.\n",
    "   - The full linkage structure is stored in a **linkage matrix (477 √ó 4)**.\n",
    "\n",
    "**Visualize with a Truncated Dendrogram**\n",
    "   - Since the vocabulary is large (478 words), we plot only the **top 25 clusters**.\n",
    "   - The dendrogram shows how words recursively merge into larger groups based on their contextual similarity.\n",
    "\n",
    "This provides an interpretable view of the Brown corpus structure and prepares us for learning more advanced embedding models such as **Word2Vec** and **GloVe**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß≠ Summary\n",
    "\n",
    "| Step                     | What It Does                                                   |\n",
    "|-------------------------|----------------------------------------------------------------|\n",
    "| Corpus                  | Loads 50 sentences from the Brown Corpus and normalizes text   |\n",
    "| Vocabulary              | Extracts all unique words (478 tokens) and builds index maps   |\n",
    "| Co-occurrence Matrix    | Counts how often words appear near each other using a ¬±2 window |\n",
    "| Hierarchical Clustering | Uses Ward‚Äôs method to group words by contextual similarity     |\n",
    "| Dendrogram Visualization| Shows the top-level clusters using a truncated dendrogram      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing the Hierarchical Clustering with a Dendrogram**\n",
    "\n",
    "After computing the hierarchical clustering using Ward‚Äôs method, we visualize the structure with a truncated dendrogram. Because the \n",
    "vocabulary contains 478 words, plotting the full dendrogram would be too dense and unreadable. Instead, we display the top 25 clusters \n",
    "using a truncated view.\n",
    "\n",
    "**Code Recap**\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=vocab,\n",
    "    truncate_mode=\"lastp\",\n",
    "    p=25,\n",
    "    leaf_rotation=0,\n",
    "    leaf_font_size=10\n",
    ")\n",
    "plt.title(\"Truncated Dendrogram ‚Äì Hierarchical Clustering of Brown Corpus Words\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Cluster Groups\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**What Is a Dendrogram?**\n",
    "\n",
    "A dendrogram is a tree-like diagram that shows how clusters are formed step-by-step during hierarchical clustering. \n",
    "   - Each leaf represents a word or cluster. \n",
    "   - Leaves that merge at low distances share similar co-occurrence patterns. \n",
    "   - Tall horizontal lines indicate merges between clusters that are more dissimilar.\n",
    "   - The x-axis shows distance (dissimilarity level).\n",
    "   - The y-axis shows cluster groups.\n",
    "\n",
    "**What This Tells Us**\n",
    "   - Words that occur in similar contexts are grouped early (small distance).\n",
    "   - More general or widely-used words tend to merge later at larger distances.\n",
    "   - The tree structure shows how clusters grow from small groups into broader categories as the similarity threshold relaxes.\n",
    "\n",
    "This visualization confirms that the co-occurrence matrix captures rich distributional relationships, even before embedding models are trained.\n",
    "\n",
    "| Component        | Role                                             |\n",
    "| ---------------- | ------------------------------------------------ |\n",
    "| `linkage_matrix` | Stores merge operations based on Ward‚Äôs method   |\n",
    "| `dendrogram()`   | Visualizes the top-level cluster hierarchy       |\n",
    "| X-axis           | Dissimilarity between merged clusters            |\n",
    "| Y-axis           | Cluster indices (truncated to top 25 groups)     |\n",
    "| Short branches   | Words with highly similar co-occurrence behavior |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   - A full dendrogram with 478 leaves becomes unreadable and slow to render.\n",
    "   - Truncating to the top 25 clusters preserves the global structure while avoiding visual clutter. This is a practical trade-off between completeness and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Where GloVe Fits Into This Workflow**\n",
    "\n",
    "Although this notebook does **not** implement GloVe training directly, it is important to understand how the steps we completed relate to the GloVe model.\n",
    "\n",
    "**What GloVe Is**\n",
    "GloVe (Global Vectors) is a word embedding method that learns dense vector representations of words using the global co-occurrence matrix of a corpus.\n",
    "\n",
    "The model was introduced in 2014 by Pennington, Socher, and Manning at Stanford University.\n",
    "\n",
    "**How It Relates to Our Notebook**\n",
    "The co-occurrence matrix we constructed in Step 4 is exactly the kind of data GloVe uses during training. GloVe takes this matrix and optimizes word vectors so that:\n",
    "   - words with similar co-occurrence patterns - have similar embeddings  \n",
    "   - rare or noisy co-occurrences - receive lower influence via the weighting function  \n",
    "\n",
    "Our pipeline so far builds the foundational components needed for GloVe:\n",
    "   - cleaned text  \n",
    "   - vocabulary  \n",
    "   - word-index mappings  \n",
    "   - co-occurrence matrix  \n",
    "\n",
    "**Why We Do Not Train GloVe Here**\n",
    "Training a full GloVe model requires:\n",
    "   - large corpora\n",
    "   - long training times\n",
    "   - specialized C extensions or optimized libraries\n",
    "\n",
    "This workshop instead focuses on understanding the steps that lead up to embedding models such as GloVe and Word2Vec, and visualizing the structure \n",
    "captured by co-occurrence statistics using hierarchical clustering.\n",
    "\n",
    "**Summary**\n",
    "The clustering performed earlier shows why models like GloVe work: they rely on the same distributional patterns that we visualized through \n",
    "the dendrogram. This provides an intuitive understanding of how embedding models ultimately represent word similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Steps to align a pretrained GloVe embedding with your vocabulary:*\n",
    "\n",
    "   - Preprocess the text data and build the vocabulary (as done earlier using the Brown corpus).\n",
    "   - Create a word-to-index dictionary so each word in the vocabulary has a unique integer ID.\n",
    "   - Load a pretrained GloVe file of a chosen vector dimension (e.g., 50D, 100D, 300D).\n",
    "   - For each word in your vocabulary, check whether it appears in the GloVe file.  \n",
    "   - If it exists, copy its pretrained vector into the embedding_matrix at the corresponding index.  \n",
    "   - If it does not exist, initialize its embedding randomly or using a default strategy.\n",
    "\n",
    "This process maps your vocabulary to the pretrained GloVe vector space, allowing you to use high-quality embeddings while keeping your own word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (478, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Pretrained Embeddings (http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "def load_glove_for_vocab(filepath, vocab, word_to_id, embedding_dim):\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Initialize embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            word = parts[0]\n",
    "            vector = parts[1:]\n",
    "\n",
    "            # Only store vectors for words that are in our vocabulary\n",
    "            if word in word_to_id:\n",
    "                idx = word_to_id[word]\n",
    "                embedding_matrix[idx] = np.asarray(vector, dtype=np.float32)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# ‚≠ê Path is one level up in ../data/\n",
    "glove_path = \"../data/glove.6B.50d.txt\"\n",
    "embedding_dim = 50\n",
    "\n",
    "# Build embedding matrix for your vocabulary\n",
    "embedding_matrix_vocab = load_glove_for_vocab(\n",
    "    glove_path, vocab, word_to_id, embedding_dim\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix_vocab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GloVe Embedding Matrix Summary**\n",
    "\n",
    "The pretrained GloVe 50-dimensional vectors were successfully aligned with the vocabulary extracted from the Brown corpus. The resulting embedding matrix has shape (478 √ó 50), where each row corresponds to a word in our vocabulary.\n",
    "\n",
    "   - Words present in the GloVe dataset receive their pretrained embeddings.\n",
    "   - Words absent from the GloVe file retain their default zero initialization.\n",
    "\n",
    "This step demonstrates how global word co-occurrence statistics (captured by GloVe) can be integrated into our NLP pipeline after constructing the\n",
    "vocabulary and co-occurrence matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   - Some Brown corpus words do not appear in the GloVe vocabulary and therefore receive zero vectors. This affects downstream similarity calculations and is a known limitation when aligning pretrained embeddings to a custom vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing Word2Vec and GloVe**\n",
    "\n",
    "| Feature             | Word2Vec                                       | GloVe                                                  |\n",
    "|---------------------|------------------------------------------------|--------------------------------------------------------|\n",
    "| Learning Approach   | Predictive model: learns embeddings by         | Count-based model: factorizes global                   |\n",
    "|                     | predicting a word from its context (Skip-Gram) | word‚Äìword co-occurrence matrix                         |\n",
    "|                     | or predicting context from a word (CBOW)       |                                                        |\n",
    "| Data Used           | Local context windows                          | Global co-occurrence statistics                        |\n",
    "| Strength            | Captures fine-grained semantic relationships   | Captures global structure and long-range patterns      |\n",
    "| Training Style      | Stochastic, online, neural network-based       | Matrix factorization + weighted least squares          |\n",
    "| Implementation      | Python (gensim), C++                           | Original: C; Python wrappers available                 |\n",
    "| Best For            | Word similarity, analogies, semantic relations | Similar tasks + stronger performance on global themes  |\n",
    "| Corpus Sensitivity  | Sensitive to local word order                  | Sensitive to overall word distribution                 |\n",
    "| Output              | Dense word embeddings                          | Dense word embeddings                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Talking Point:**  \n",
    "   - Word2Vec captures fine-grained contextual relationships because it trains on local windows, while GloVe captures global structure by factorizing \n",
    "co-occurrence statistics. This is why the two models often produce complementary results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: The Workshop**\n",
    "\n",
    "At the end of this session, push your final Jupyter notebook to GitHub and submit the public `.git` URL to the assignment dropbox before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Push the final notebook to GitHub and send the `.git` URL to the **assignment dropbox** before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "   - Work in **teams of 2** (although evaluation is **individual**).\n",
    "   - Implement **Word2Vec** and **GloVe** using real-world text data.\n",
    "   - Develop a well-structured **Jupyter Notebook** with clear code and Markdown.\n",
    "   - Apply **Git/GitHub** for collaborative version control and code sharing.\n",
    "   - Identify and highlight **coding issues** (‚Äútalking points‚Äù) directly in Markdown.\n",
    "\n",
    "\n",
    "## üß© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** ‚Äì Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* ‚Äì NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** ‚Äì Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
